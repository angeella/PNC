---
title: "Tutorial 1: Linear regression models"
author: "Tutor: Angela Andreella"
subtitle: 'Special thank to Ilaria Bussoli for the materials'
date: "Venice, 11/11/2021"
output: 
    html_document:
        toc: TRUE
        theme: united
        highlight: tango
        # code_folding: hide
fontsize: 11pt
geometry: margin = 1in
---

<style type="text/css">
.main-container {
  max-width: 1100px;
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, fig.align = "center", out.width = '80%', warning = F, message = F)
```

# Tutorial 1: Boston data set

Before starting the analysis, load the libraries `MASS` and `ISLR`. 

We will work with the `Boston` data set from the `MASS` library. This data set contains information collected by the U.S Census Service concerning housing in the area of Boston, MA. It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms. <br> The data was originally published by Harrison, D. and Rubinfeld, D.L. '*Hedonic prices and the demand for clean air*', J. Environ. Economics \& Management, vol.5, 81-102, 1978.

Were recorded `r library(tidyverse); MASS::Boston %>% ncol()` different characteristics on `r library(tidyverse); MASS::Boston %>% nrow()` census tracts around Boston. In particular, these features were:

- *crim* - per capita crime rate by town;
- *zn* - proportion of residential land zoned for lots over 25,000 sq.ft.;
- *indus* - proportion of non-retail business acres per town;
- *chas* - Charles River dummy variable (1 if tract bounds river; 0 otherwise);
- *nox* - nitric oxides concentration (parts per 10 million);
- *rm* - average number of rooms per dwelling;
- *age* - proportion of owner-occupied units built prior to 1940;
- *dis* - weighted mean of distances to five Boston employment centres;
- *rad* - index of accessibility to radial highways;
- *tax* - full-value property-tax rate per \$10,000;
- *ptratio* - pupil-teacher ratio by town;
- *black* - $1000(Bk - 0.63)^2$, where $Bk$ is the proportion of blacks by town;
- *lstat* - lower status of the population (percent);
- *medv* - median value of owner-occupied homes in \$1000's.

## Exploratory data analysis

This is a crucial part and usually takes up most of the time. A proper and extensive exploratory data analysis (EDA) would reveal interesting patterns and help to prepare the data in a better way for the following analyses. It can be roughly summarised in 3 big parts:

1. _Structure and summary of data_ -- To check the type of variables in the data set and compute location indeces (e.g., mean, median) of the variables of interest.
2. _Exploratory plots_ -- Histograms, boxplots, barplots, correlogram or scatterplots (e.g., skeweness, outliers).
3. _"Cleaning" process_ -- Are there any NAs? Missing values? Integer variables to be converted in factors? Is there the need to create a 'test' and 'training' data sets? \dots

_How does EDA apply to the "Boston" data set?_

### 1. Structure and summary of data

We can get a glimpse of the structure of data using the function `str()`:

```{r loading_data, echo=FALSE}
# --- Libraries --- #
# Libraries for output/manipulation of data
library(tidyverse)
library(moderndive)
library(skimr)
library(infer) # not used (for now)
library(kableExtra) # not used (for now)
library(ggfortify)
# Datasets and functions
library(MASS)
library(ISLR)

# How to check for missing packages and, in case, install them
#
# if(!require(name_of_the_package)){
#     install.packages("name_of_the_package", dependencies = T)
#     library(name_of_the_package)
# }
```

```{r data_structure, eval=3:4}
library(MASS)
library(ISLR)
census_tracts <- Boston
str(census_tracts)
```

We can see that all the variables are classified as numeric, with the exception of `chas` and `rad`, which can be thought of and transformed as factors: in particular, `chas` is a dichotomous variable and it will be interpreted as a categorical variable with 2 levels ("0" and "1"), while `rad` will be interpreted as a categorical variable with 9 levels ("1", "2", "3", "4", "5", "6", "7", "8", "24"):

:::: {style="display: grid; grid-template-columns: auto auto; grid-column-gap: 5px; place-items: start;"}
::: {}
__R commands__ 

```{r data_structure_factors}
# Variable "chas"
census_tracts$chas <- as.factor(census_tracts$chas) 
levels(census_tracts$chas)
# Variable "rad"
census_tracts$rad <- as.factor(census_tracts$rad)
levels(census_tracts$rad)
```
:::
::: {}
__Comments__ 

* `as.factor(x)` -- function transforming variable `x` in a categorical one.
* `levels(x)` -- function listing all levels of variable `x`.
:::
::::

Using `summary()` on an object of class `data.frame`, we can compute basic statistics for each variable in the data set: minimum, maximum and mean values and quartiles:

```{r data_summary}
summary(census_tracts)
# # --- Other packages and functions --- #
# # 
# # The following code uses "skimr" or "summarytools" libraries to print the summary() table in a more elegant format. 
# # It is possible to include in the output table the histograms/barplots of the distributions of the variables as a plus.
# 
# # 1) Package: skimr
# library(skimr)
# fix_windows_histograms()   # prepare the space where to insert histograms of variable distributions
# census_tracts %>% skim()   # use function skim(), together with "dplyr" (already loaded with "tidyverse" library),
#                            # to print the table
#
# # 2) Package: summarytools
# library(summarytools)
# df_table <- dfSummary(x = census_tracts,      # working data set 
#                       style = "grid",         # style of the table
#                       graph.magnif = 0.75,    # dimension of inserted graphs
#                       valid.col = F,          # include column indicating count/proportion of non-missing values?
#                       plain.ascii = F)        # to use markdown rendering
# print(df_table, method = "render)             # print the table
```

### 2. Exploratory plots

Variables `crim` and `black` take on a wide range of values. Moreover, the difference between the respective median and mean values is high, as for the variables `zn` and `rm`, which indicates lot of outliers and a skewed distribution, as can it be seen also from the histograms below (code displayed only for `crim`, `zn` and `age` variables).

So, we make histograms plot for each quantitative variable. We can use base `R` functions:

```{r histograms0, fig.height=3}
var_interest_numeric <- colnames(census_tracts)[!(colnames(census_tracts) %in% c("chas", "rad"))]
for(i in c(1:length(var_interest_numeric))){
  
  hist(census_tracts[,var_interest_numeric[i]], main = var_interest_numeric[i], xlab = var_interest_numeric[i])
}
```

or we can use the packages `dplyr`, `tidyr`, `ggplot`:

```{r histograms1, fig.height=3}
census_tracts %>%
  dplyr::select(crim, zn, age) %>%
  gather(cols, value) %>%
  ggplot(aes(x = value)) + geom_histogram(bins = 50) + facet_wrap(.~ cols, ncol = 3)
```

```{r histograms2, fig.height=6, echo=FALSE}
census_tracts %>%
  dplyr::select(indus, rm, dis, ptratio, lstat, medv) %>%
  gather(cols, value) %>%
  ggplot(aes(x = value)) + geom_histogram(bins = 50) + facet_wrap(.~ cols, ncol = 3)
```
```{r histograms3, fig.height=3, echo=FALSE}
census_tracts %>%
  dplyr::select(tax, black) %>%
  gather(cols, value) %>%
  ggplot(aes(x = value)) + geom_histogram(bins = 50) + facet_wrap(.~ cols, ncol = 3)
```
```{r histograms4, fig.height=3, echo=FALSE}
census_tracts %>%
  ggplot(aes(x = nox)) + geom_histogram(bins = 50) 
```
```{r boxplot1, fig.height=3, echo=FALSE}
census_tracts %>%
  ggplot(aes(x = rad)) + geom_bar(stat = "count")
```
```{r boxplot2, fig.height=3, echo=FALSE}
census_tracts %>%
  ggplot(aes(x = chas)) + geom_bar(stat = "count")
```

> **_Question:_** _Are there any missing values or NAs?_

We can check the correlation between the *quantitative* variables to look at the level of linear dependence between pairs of two variables. We can either 

(a) Read the correlation matrix (since the correlation matrix is symmetric -- $COR(X,Y) = COR(Y,X)$ --, we can simply look at the upper or lower triangular version of it).

:::: {style="display: grid; grid-template-columns: 1fr 2fr; grid-column-gap: 20px; place-items: start;"}
::: {}
__R commands__ 

```{r correlation1}
# Correlation matrix
corr_matrix <- round(cor(census_tracts[,!(colnames(census_tracts) %in% c("chas", "rad"))]), 2)
corr_matrix[lower.tri(corr_matrix)] <- 0
corr_matrix
```
:::
:::{}
__Comments__ 

* First line:
  * `!(colnames(census_tracts) %in% c("chas", "rad"))` -- keep only the columns having name different from `chas` and `rad`.
  * `census_tracts[,!(...)]` -- keep the data set without `chas` and `rad` columns.
  * `cor(census_tracts[...])` -- compute the correlation matrix for all possible couples of variables in the data set.
  * `round(cor(...), 2)` -- rounds to two decimal places all correlation values.
* Second line: keep only the values in the upper triangular part of the matrix for a better reading.
:::
::::

(b) Plot the correlation matrix through a correlogram (higher absolute values of correlation between pairs of variables correspond to more vibrant colors on the cells of the correlogram for those pairs).

:::: {style="display: grid; grid-template-columns: 1fr 2fr; grid-column-gap: 20px; place-items: start;"}
::: {}
__R commands__ 

```{r correlation2, fig.width=14, fig.height=14}
# Correlogram
library(ggcorrplot)                                             
ggcorrplot(corr_matrix, type = "upper", lab = T, lab_size = 7, outline.col = "white", 
           colors = c("tomato2", "white", "springgreen3"), title = "", ggtheme = theme_gray, 
           p.mat = cor_pmat(corr_matrix), pch.cex = 30, tl.cex = 20)
```
:::
:::{}
__Comments__ 

* Install and load the library `ggcorrplot` to plot the correlogram. 
* The function to be used is `ggcorrplot(x)`, where `x` is the correlation matrix (basic usage).
* In particular:
  * `corr_matrix` -- the first argument to be passed is the correlation matrix.
  * `type = "upper"` -- print only the upper triangular part of the matrix.
  * `lab = T` -- add correlation values on the plot. 
  * `lab_size = 7` -- dimension of correlation values on the plot.
  * `outline.col = "white"` -- border color of the cells of the correlation values.
  * `colors = c("tomato2", "white", "springgreen3")` -- vector of 3 colors for negative, mid and positive correlation values.
  * `title = ""` -- no title of the plot.
  * `ggtheme = theme_gray` -- ggplot2 function or theme object
  * `p.mat = cor_pmat(corr_matrix)` -- matrix of p-values, computed using the function `cor_pmat()`, from the hypothesis testing procedure with $H_0: correlation = 0$ vs $H_1: correlation \neq 0$. All those correlation values for which the p-value of the test was bigger than significance level $\alpha = 0.05$ are crossed.
  * `pch.cex = 30` -- size of symbol for not statistically significant correlation values.
  * `tl.cex = 20` -- size of variable name labels.
:::
::::

*General note:* If you use a function inside a package only one time, and you don't want to occupy memory when loading all the functions inside that package, once you have installed the latter you can simply call the function of interest by `package::function(...)`. For example, if we only want to use the `ggcorrplot` function from the homonymous package, we can simply use `ggcorrplot::ggcorrplot(x)`.

However, to speed up the code's writing we can use the following function from the `stats` package which is a basic package in `R`:

```{r correlation3, fig.width=14, fig.height=14}
heatmap(corr_matrix)
```

or the `corrplot` function from the `corrplot` package:

```{r correlation4, fig.width=14, fig.height=14}
corrplot::corrplot(corr_matrix)
```


Another way to analyse the relationships between numerical variables is through the graphical explotation of the scatterplots. 

```{r scatterplot_matrices, fig.height=14, fig.width=14}
# We drop the following variables because they are qualitative or contain a lot of outliers
dropList <- c("chas", "rad", "crim", "zn", "black") 
# We keep the indeces of variables not in the "drop" list
indeces <- which(!(colnames(census_tracts) %in% dropList))
# We plot a matrix of scatterplot using the "splom" function from the "lattice" library
lattice::splom(x = census_tracts[, indeces], pch = "*")
# We can imitate the latter plot using also basic R:
pairs(census_tracts[, indeces], pch = "*", col = "#0080ff")
```

> **_Question:_** _What type or relationships can we detect?_

### 3. "Cleaning" process

We can also prepare training and test data sets for prediction for (eventual) future analyses:

```{r train_test}
# Number of total rows in the data set (= maximum sample size from which we can draw random units):
n <- nrow(census_tracts)
# Size of the training data set (3/4 of "census_tracts"):
size <- round(0.75 * n)
# We set a seed for reproducibility of our results:
set.seed(666)
# We sample the row indeces representing the statistical units that will end up in the training set:
row.ind <- sample(x = 1:n, size = size) 
train_ct <- census_tracts[row.ind,]
# The remaining row indeces corresponds to the statistical units that end up in the test set:
test_ct <- census_tracts[-row.ind,]
# Check on factors (we must have all levels of categorical variables in both training and test sets):
all.equal(levels(train_ct$chas), levels(test_ct$chas))
all.equal(levels(train_ct$rad), levels(test_ct$rad))
```

_A little comment:_ When we split the original data set in training and test and there are categorical variables, it is better to check that all levels/categories of those variables are present in both training and test data. 

> **_Question:_** _What could happen if categorical variable $X$ has all its levels in the training set but not in the test set? And if it has all its levels in the test set, but not the training one?_

## Simple linear regression

### 1. The model

We will start by fitting a simple linear regression model, with `medv` as the response variable and `lstat` as the predictor: 
\[
  \text{EQ. 1:} \qquad \text{medv}_i \approx \beta_0 + \beta_1 \, \text{lstat}_i, \qquad i = 1, \dots, n.
\]
The interest lies in testing the coefficients of the linear model, specifically $\beta_1$:

- The null hypothesis is that the coefficient associated with `lstat` is zero -- $H_0: \beta_1 = 0$.
- The alternative hypothesis is that the coefficient associated with `lstat` is not zero -- $H_1: \beta_1 \neq 0$ $\longrightarrow$ there exists a relationship between (the dependent variable) `medv` and (the independent variable) `lstat`.
- If the $p$-value associated to the estimate of $\beta_1$ is less than 0.05, we usually reject the null hypothesis.

To see detailed results of the model, we can use the `summary` method for class `lm()`, the class of objects representing linear models in <tt>R</tt> (one of them): this gives us $p$-values and standard errors for all coefficients in the model, as well as the $R^2$ statistic and $F$-statistic: the former measures the proportion of the variation in the dependent variable explained by all of independent variables in the model, while, if the latter is statistically significant, the model can be considered "good". 

:::: {style="display: grid; grid-template-columns: auto 2fr; grid-column-gap: 20px; place-items: start;"}
::: {}
__R commands__

```{r slr1}
# Simple linear regression model:
lm.fit.simple <- lm(formula = medv ~ lstat, 
                    # formula equivalent to the model in "EQ. 1"
                    data = census_tracts)
summary(lm.fit.simple)
```
:::
::: {}
__Comments on `summary(lm.fit.simple)` output__

* `Call` section -- print the code of the linear regression model.
* `Residuals` section -- Location indeces of the residuals of the model, computed as $$e_i = y_i - \hat{y}_i = \text{medv}_i - \widehat{\text{medv}}_i,$$ for $i = 1, \dots, n$.
* `Coefficients` section -- Matrix of dimension "number of coefficients" $\times$ 4, where the 4 columns are 
  * `Estimate` -- estimates of $\beta_0$ and $\beta_1$, obtained through OLS (ordinary least squares). Let us use $\hat{\beta}_0$ and $\hat{\beta}_1$ as symbols for estimates of $\beta_0$ and $\beta_1$, respectively.
  * `Std. Error` -- standard errors for the estimates of $\beta_0$ and $\beta_1$.
  * `t value` -- t-statistic value corresponding to the tests $$H_0: \beta_0 = 0 \text{ vs } H_1: \beta_0 \neq 0$$ and $$H_0: \beta_1 = 0 \text{ vs } H_1: \beta_1 \neq 0.$$
  * `Pr(>|t|)` -- t-statistics related (two-sided) p-values.
* Other quantities, as $R^2$ and $F$-statistics.
:::
::::

> **_Question:_** _Is the coefficient associated to_ `lstat` _statistically significant? For which significance level $\alpha$? And what can we say about the model we just fitted? Is it good?_

### 2. Interpretation of the results 

The fitted regression line can be written as 
\[ 
\widehat{\text{medv}}_i = \hat{\beta}_0 + \hat{\beta}_1 \, \text{lstat}_i = 34.55 - 0.95 \, \text{lstat}_i, \qquad i = 1, \dots, n. 
\]

We can say that the coefficient associated to `lstat` is statistically significant at a level $\alpha = 0.05$ (actually, at level $\alpha = 0.001$), meaning that there exists a (strong) relationship between `medv` and `lstat`. Moreover, since it has a negative sign, we can interpret it as follows: if `lstat` differed by 1\%, `medv` will decrease by `r abs(round(coef(lm.fit.simple)[[2]], 2))` units, on average (e.g., 11\% of lower status population would, on average, live in owner-occupied homes having a median value of \$950 less than what would be for a 10\% percentage of lower status population). 

We can use the `names()` function in order to find out what other pieces of information are stored in `lm.fit.simple`, or we can extract them using specific extractor functions. <br> Usually, we are interested in the estimated coefficients, the residuals or the fitted values, which can be also extracted by `summary(lm.fit.simple)`(try `str(summary(lm.fit.simple))` -- use also the <tt>R</tt> help --).

```{r slr2a, echo=1:3}
# Information stored in "lm.fit.simple"
names(lm.fit.simple)
# Coefficients (estimates)
results <- as.data.frame(t(c(lm.fit.simple$coefficients, rep(" ", 10), coef(lm.fit.simple))), stringAsFactors = F)
kable(results, row.names = F, col.names = c(names(lm.fit.simple$coefficients), rep(" ", 10), names(coef(lm.fit.simple)))) %>%
  kable_paper("hover", full_width = F) %>%
  row_spec(row = 0:1, monospace = T) %>%
  add_header_above(c("lm.fit.simple$coefficient" = 2, " " = 10, "coef(lm.fit.simple)" = 2), monospace = T)
```
```{r slr2b}
# Confidence intervals
confint(lm.fit.simple)
```

> **_Question:_** _What is the interpretation of the 95\% confidence interval for the coefficient $\beta_1$ associated to_ `lstat`_?_

### 3. Prediction and confidence intervals for `medv`, the response variable

The `predict()` function can be used to produce confidence intervals and prediction intervals for the prediction of `medv` for a given value of `lstat`: the result is a matrix, with number of rows equal to the number of new values of `lstat` we pass to `predict`, and 3 columns, containing in the column 

* `fit` -- the estimated/predicted value of `medv` for a given value of `lstat`.
* `lwr` -- the lower extreme of the confidence/prediction intervals containing `medv` for a given value of `lstat`.
* `upr` -- the upper extreme of the confidence/prediction intervals containing `medv` for a given value of `lstat`.

Remember that (slides 27 and 28 of prof. Gaetan part):

* A 95\% confidence interval for the __coefficients of the simple linear regression__ is computed as 
\[ 
  \hat{\beta}_0 \pm 1.96 \cdot SE(\hat{\beta}_0) 
\] 
for $\beta_0$, or 
\[ 
  \hat{\beta}_1 \pm 1.96 \cdot SE(\hat{\beta}_1)
\] 
for $\beta_1$.
* A 95\% confidence interval for the __average response__ of the model regards $E(\text{medv})$.
* A 95\% prediction interval for the __individual response__ of the model regards $\text{medv}_{\text{new data}}$.

As an example:

```{r slr3}
# Confidence intervals
predict(lm.fit.simple,                                       # lm fitted object
        newdata = data.frame(lstat = (c(5,10,15))),          # new values for lstat (to be given as data.frame)
        interval = "confidence")                             # type of interval
# Prediction intervals
predict(lm.fit.simple,                                       # lm fitted object
        newdata = data.frame(lstat = (c(5,10,15))),          # new values for lstat (to be given as data.frame)
        interval = "prediction")                             # type of interval
```

For instance, the 95\% confidence interval associated with a `lstat` value of 10 is (24.47, 25.63), and the 95\% prediction interval is (12.828, 37.28). As expected, the confidence and prediction intervals are centered around the
same point (a predicted value of 25.05 for `medv` when `lstat` equals 10), but the latter are substantially wider.

### 4. Graphical representation of the model

The plot of the regression model with both confidence and prediction intervals can be obtained as follows:

```{r slr4}
# New data
new_lstat <- seq(min(census_tracts$lstat), max(census_tracts$lstat), by = 0.05)
# 95% confidence interval for regression line
conf_interval <- predict(lm.fit.simple, 
                         newdata = data.frame(lstat = new_lstat), 
                         interval = "confidence", level = 0.95)
# 95% prediction interval for regression line
pred_interval <- predict(lm.fit.simple, 
                         newdata = data.frame(lstat = new_lstat), 
                         interval = "prediction", level = 0.95)

# Plot
plot(x = census_tracts$lstat,                                         # `lstat` on x-axis
     y = census_tracts$medv,                                          # `medv` on y-axis
     pch = "*", cex = 0.8,                                            # graphical parameter
     xlab = "lower status of the population (in %)",                  # labels of x-axis and y-axis
     ylab = "median value of owner-occupied homes (in $1000's)")
abline(lm.fit.simple, lwd = 3, col = "blue")                          # regression line
lines(new_lstat, conf_interval[,2], col = "blue", lty = 2)            # lower extreme conf. interval
lines(new_lstat, conf_interval[,3], col = "blue", lty = 2)            # upper extreme conf. interval
lines(new_lstat, pred_interval[,2], col = "red", lty = 2)             # lower extreme pred. interval
lines(new_lstat, pred_interval[,3], col = "red", lty = 2)             # upper extreme pred. interval
legend("topright",                                                    # legend
       title = expression(1-alpha == 0.95 ~ ("95%")),
       legend = c("Conf. interval", "Pred. interval"),
       lty = 2, col = c("blue", "red"))
# # Alternatively:
# temp <- predict(lm.fit.simple, interval = "prediction")
# new_df <- cbind(census_tracts, temp)
# new_df %>%
#   ggplot(aes(x = lstat, y = medv)) + 
#   geom_point() + 
#   labs(x = "lower status of the population (in %)", y = "median value of owner-occupied homes (in $1000's)") +
#   stat_smooth(method = "lm", se = T) + 
#   geom_line(aes(y = lwr), color = "red", linetype = "dashed", lwd = 1) +
#   geom_line(aes(y = upr), color = "red", linetype = "dashed", lwd = 1)
```

### 5. Diagnostics

There is some evidence for non-linearity in the relationship between `lstat` and `medv`. In particular, variable `medv` seems to be censored at 50.00 (corresponding to a median price of \$50,000). Censoring is suggested by the fact that the highest median price of exactly \$50,000 is reported in 16 cases (16 points in the scatterplot above, top-left corner), while 15 cases have prices between \$40,000 and \$50,000, with prices rounded to the nearest hundred.

This non-linearity can be examineed also through some diagnostic plots. In order:

* **Top-left plot** -- **Residual versus fitted values** <br> This plot shows if residuals have non-linear behaviour, which may happen in case of a non-linear relationship between the predictor and the response. If you find equally spread residuals around a horizontal line in 0 without distinct patterns, that is a good indication you do not have non-linear relationships. 
* **Top-right plot** -- **Normal Q-Q plot** <br> This plot shows if residuals are normally distributed. It is a scatterplot created by plotting the theoretical quantiles of a standard normal distribution against the quantiles of the standardized residuals: if both sets of quantiles came from the same distribution (i.e., standard normal), we should see the points forming a line that is roughly straight. 
* **Bottom-left plot** -- **Scale-Location comparison** <br> This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of homoscedasticity in your data (homoscedasticity means equal variance). If you see a horizontal line in 1 with equally (randomly) spread points, than the situation is good.
* **Bottom-right plot** -- **Residuals vs Leverage** <br> This plot helps us to find influential cases (i.e., subjects) if any. Not all outliers are influential in linear regression analysis: even though data have extreme values, the regression results wouldn't be much different if we either include or exclude them from analysis. On the other hand, some cases could be very influential even if they look to be within a reasonable range of the values. To detect those values, we observe if there are values at the upper/lower right corner of this graph, outside of a dashed line called Cook's distance: when this happens, the cases are influential to the regression results, meaning that the regression results will be altered if we exclude them.

:::: {style="display: grid; grid-template-columns: auto auto; grid-column-gap: 20px; place-items: start;"}
::: {}
__R commands__

```{r slr5, fig.height=10, fig.width=10, echo=c(1,2,3,5,6)}
# Diagnostic plots
par(mfrow = c(2,2))
plot(lm.fit.simple)
par(mfrow = c(1,1))
# # Alternatively
# autoplot(lm.fit.simple, label.size = 3)
```
:::
::: {}
__Comments__

* **Residual versus fitted values** -- We can see a parabolic pattern in our case, meaning that there is a non-linear relationship that was not explained by the model and was left out in the residuals.
* **Normal Q-Q plot** -- The points do not follow the straight line, meaning that the standardized residuals probably not follow a standard normal distribution (i.e., the residuals are not normal).
* **Scale-Location** -- The residuals seem to spread wider the closer it gets to 0 along the x-axis. Because of that, the red smooth line is not horizontal and shows a parabolic behaviour.
* **Residuals vs Leverage** -- This is a typical case when there are no influential observations. The Cook's distance lines (i.e., red dashed lines) are not seen because all cases are well inside of it. 
:::
::::

## Multiple linear regression

We can add more variables to the model, keeping in mind that now we have both quantitative and qualitative variables to take into consideration as predictors. Up to now we consider only dummy (dichotomous) variables between the qualitative's we have:
\[
  \text{medv}_i \approx \beta_0 + \beta_1 \, \text{lstat}_i + \beta_2 \text{age}_i + \dots + \beta_{12} \text{chas}_i, \qquad i = 1, \dots, n.
\]
Below we use the syntax `formula = response variable ~.` inside `lm()` to include all the variables except `medv` in the model as linear independent predictors, while `-rad` after the symbol `~` permits to not consider the `rad` variable into the model.

__Question:__ _What does we find? How can we interpret the results of the regression models? And in terms of goodness of the model, what does the $F$-statistic tell us?_

```{r mlr1, echo=1:2}
lm.fit.multiple <- lm(medv ~. -rad, data = census_tracts)
summary(lm.fit.multiple)
# car::vif(lm.fit.multiple)
```

Coefficients associated to variables as `indus`, `age` and `tax` result as not-statistically significant at a significance level $\alpha = 0.05$.

__Question:__ _If we change the reference level for `rad`, may we see different results?_

```{r mlr2}
census_tracts2 <- census_tracts
census_tracts2$rad <- relevel(census_tracts2$rad, ref = "24")
lm.fit.multiple2 <- lm(medv ~., data = census_tracts2)
summary(lm.fit.multiple2)
```

## Interaction terms

It is easy to include interaction terms in a linear model using the `lm()` function.

```{r it}
lm.fit.interactions <- lm(medv ~ lstat*age, data = census_tracts)
summary(lm.fit.interactions)
```

__Question:__ _What does we find? How can we interpret the results of the regression models? And in terms of goodness of the model, what does the $F$-statistic tell us?_

## Non-linear transformations of the predictors

The `lm()` function can also accommodate non-linear transformations of the predictors. For instance:

```{r nonLinearVar1}
lm.fit.nonlinear <- lm(medv ~ lstat + I(lstat^2), data = census_tracts)
summary(lm.fit.nonlinear)
```

The near-zero $p$-value associated with the quadratic term suggests that it leads to an improved model. We use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit:

```{r nonLinearVar2}
anova(lm.fit.simple, lm.fit.nonlinear)
```

The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the model with both linear and quadratic terms is superior. Here the $F$-statistic is 135 and the associated $p$-value is virtually zero. This provides very clear evidence that the model containing the predictors `lstat` and `lstat`$^2$ is far superior to the model that only contains the predictor `lstat`.

__Question:__ _Is this surprising or not? And what can we say from the diagnostics below?_

```{r nonLinearVar3, fig.height=10, fig.width=10, echo=c(1,2,4,5)}
par(mfrow = c(2,2))
plot(lm.fit.nonlinear)
par(mfrow = c(1,1))
# # Alternatively
# autoplot(lm.fit.nonlinear, label.size = 3)
```

# Exercises

The following questions involve the use of the `Auto` data set.

(a) Explore the data set keeping in mind the 3 points of the EDA process and comment.
(b) Use the `lm()` function to perform a simple linear regression with `mpg` as the response and `horsepower` as the predictor. Print the results and comment on the output. For example:
    * Is there a relationship between the predictor and the response?
    * How strong is the relationship between the predictor and the response?
    * Is the relationship between the predictor and the response positive or negative?
    * What is the predicted `mpg` associated with a `horsepower` of 98? What are the associated 95\% confidence and prediction intervals?
(c) Plot, in a unique graph, the response against the predictor and display the least squares regression line.
(d) Execute diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.
(e) Produce a scatterplot matrix and relative plot, both including all variables in the data set.
(f) Compute the matrix of correlations between all the variables. What problems do you encounter? Solve the problem and comment.
(g) Perform a multiple linear regression with `mpg` as the response and all other variables except `name` as the predictors. Print the results. Comment on the output. For instance:
    * Is there a relationship between the predictors and the response?
    * Which predictors appear to have a statistically significant relationship to the response?
    * What does the coefficient for the `year` variable suggest?
(h) Now update the previous regression models, adding as a predictor also `name`. Provide an interpretation of the related coefficient(s) in the model.
(i) Produce diagnostic plots of the linear regression fit obtained in point (h). Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?
(j) Fit linear regression models with interaction effects of your choice (try to find interesting or likely interactions). Do any interactions appear to be statistically significant?
(k) Try a few different transformations of the variables, such as $\log(X)$, $\sqrt{X}$, or $X^2$, in a case of multiple linear regression on a subset of interest. Comment on/plot your findings.
