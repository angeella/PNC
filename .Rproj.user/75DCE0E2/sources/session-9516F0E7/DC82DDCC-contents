---
title: "Moving beyond linearity"
author: "Angela Andreella"
institute: "Ca' Foscari University of Venice"
date: "03/10/2022"
output: 
    html_document:
        toc: TRUE
        number_sections: TRUE
        theme: united
        highlight: tango
        css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, fig.align = "center", out.width = '100%', error = TRUE, class.error   = "bg-danger", warning = F)
```

Before starting, check if you have the following packages: `caret, splines, gam, mgcv, boot`. If not, install them.

# Practical case: `Boston` data set

We will work with the `Boston` data set from the `MASS` library. This data set contains information collected by the U.S Census Service concerning housing in the area of Boston, MA. It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms. <br> The data was originally published by Harrison, D. and Rubinfeld, D.L. '*Hedonic prices and the demand for clean air*', J. Environ. Economics \& Management, vol.5, 81-102, 1978.

Were recorded `r library(tidyverse); MASS::Boston %>% ncol()` different characteristics on `r library(tidyverse); MASS::Boston %>% nrow()` census tracts around Boston. In particular, these features were:

- *crim* - per capita crime rate by town;
- *zn* - proportion of residential land zoned for lots over 25,000 sq.ft.;
- *indus* - proportion of non-retail business acres per town;
- *chas* - Charles River dummy variable (1 if tract bounds river; 0 otherwise);
- *nox* - nitric oxides concentration (parts per 10 million);
- *rm* - average number of rooms per dwelling;
- *age* - proportion of owner-occupied units built prior to 1940;
- *dis* - weighted mean of distances to five Boston employment centres;
- *rad* - index of accessibility to radial highways;
- *tax* - full-value property-tax rate per \$10,000;
- *ptratio* - pupil-teacher ratio by town;
- *black* - $1000(Bk - 0.63)^2$, where $Bk$ is the proportion of blacks by town;
- *lstat* - lower status of the population (percent);
- *medv* - median value of owner-occupied homes in \$1000's.

We will predict the median house value (`medv`), in Boston suburbs, based on the predictor variable `lstat` (percentage of lower status of the population) and/or other predictors.

We will randomly split the data into training set (75\% for building a predictive model) and test set (25\% for evaluating the model). 

```{r loading_data_libs, message=FALSE}
## Libraries ##
library(tidyverse)
theme_set(theme_bw())
library(caret)
library(kableExtra)
library(MASS)
library(ISLR)
library(splines)
library(gam)
library(mgcv)
library(boot)

## Data Loading ##
#
# Load the data
data("Boston", package = "MASS")
#
# - Split the data into training and test set.
# - Check also factor variable!
set.seed(123)
training.samples <- Boston$medv %>% createDataPartition(p = 0.75, list = FALSE)
train.data  <- Boston[training.samples, ]
train.data$chas <- as.factor(train.data$chas)
train.data$rad <- as.factor(train.data$rad)
train.data$rad <- relevel(train.data$rad, ref = "24")
test.data <- Boston[-training.samples, ]
test.data$chas <- as.factor(test.data$chas)
test.data$rad <- as.factor(test.data$rad)
test.data$rad <- relevel(test.data$rad, ref = "24")
```

# Models: `lstat` as only predictor for `medv`

We visualize again the scatter plot of `medv` against `lstat`, to have an idea of the type of relation between the predictor and the response variable.

```{r plot_couple}
ggplot(train.data, aes(x = lstat, y = medv) ) +
  geom_point(pch = 17, size = 2, col = "darkorange2") +
  labs(x = "% lower status",
       y = "median value of owner-occupied homes ($1000s)") + 
  theme(axis.title.y = element_text(vjust = 3.5),
        axis.title.x = element_text(vjust = -1.5))
```

## Linear regression 

The first model we implement is a linear model: our idea is to "prove" that its behaviour will be worst with respect to non-linear models.

The model formula is
\[
    \text{medv}_i = \beta_0 + \beta_1 \ \text{lstat}_i + \varepsilon_i, \qquad i = 1, \dots, n,
\]
where $n$ represents the number of units in the training data set ($n = `r nrow(train.data)`$).

```{r lm1}
## Linear regression model ##
#
# -- The model
model.lm <- lm(medv ~ lstat, data = train.data)
summary(model.lm)
# -- The plot
ggplot(train.data, aes(x = lstat, y = medv) ) +
  geom_point(pch = 17, size = 2, col = "darkorange2") +
  labs(x = "% lower status",
       y = "median value of owner-occupied homes ($1000s)") + 
  theme(axis.title.y = element_text(vjust = 3.5),
        axis.title.x = element_text(vjust = -1.5)) + 
  stat_smooth(method = lm, formula = y ~ x)
```

We can also do the previous plot using the basic `plot` function:

```{r}
conf_interval <- predict(model.lm, interval="confidence",
                         level = 0.95)
plot(train.data$lstat, train.data$medv)
abline(model.lm, col="red")
lines(train.data$lstat, conf_interval[,2], col="blue", lty=2)
lines(train.data$lstat, conf_interval[,3], col="blue", lty=2)
```


Since we want to compare different models, we must resort to the techniques seen in the previous tutorial to select the model that best represents the relationship between `lstat` and `medv`; therefore, we will use the test set to predict the new `medv` values by the fitted models and calculate different metrics to compare the models.

```{r lm2}
## Fit the model ##
pred.lm <- model.lm %>% predict(test.data)
```

```{r lm3}
## Model performance ##
# -- RMSE, R2 and MAE are from `caret` package
performance.lm <- data.frame(
  AIC = round(AIC(model.lm),4),
  BIC = round(BIC(model.lm),4),
  RMSE = RMSE(pred = pred.lm, obs = test.data$medv),
  R2 = R2(pred = pred.lm, obs = test.data$medv),
  MAE = MAE(pred = pred.lm, obs = test.data$medv)
)
```

## Polynomial regression 

The model formula is
\[
    \text{medv}_i = \beta_0 + \beta_1 \ \text{lstat}_i + \beta_2 \ \text{lstat}_i^2 + \ldots + \beta_d \ \text{lstat}_i^d + \varepsilon_i, \qquad i = 1, \dots, n,
\]
where $d$ is the degree of the polynomial function. 

Since different values of $d$ lead to different models, and we want to use the "best" of these, we must treat $d$ as a tuning parameter. We choose the optimal $d$, $d^{opt}$ by leave-one-out cross validation (LOOCV) on the training data set, and fit the polynomial regression using $d^{opt}$ as degree of the polynomial function.

First of all, let's look the polynomial function in the `medv` versus `lstat` scatterplot that we saw before:

1. *Second Order* 

```{r}
p <- ggplot(train.data, aes(x = lstat, y = medv) ) +
  geom_point(pch = 17, size = 2, col = "darkorange2") +
  labs(x = "% lower status",
       y = "median value of owner-occupied homes ($1000s)") + 
  theme(axis.title.y = element_text(vjust = 3.5),
        axis.title.x = element_text(vjust = -1.5))
p + stat_smooth(method="lm", 
                formula=y ~ poly(x, 2),colour="red")
```

2. *Third Order* 

```{r}
p + stat_smooth(method="lm",
                formula=y ~ poly(x, 3),colour="red")
```

4. *Fourth Order* 

```{r}
p + stat_smooth(method="lm",
                formula=y ~ poly(x, 4),colour="red")
```

5. *Fifth Order* 

```{r}
p + stat_smooth(method="lm", 
                formula=y ~ poly(x, 5),colour="red")
```

6. *Sixth Order* 

```{r}
p + stat_smooth(method="lm", 
                formula=y ~ poly(x, 6),colour="red")
```

7. *Seventh Order* 

```{r}
p + stat_smooth(method="lm", 
                formula=y ~ poly(x, 7),colour="red")
```

We can see that after the sixth order the regression line doesn't change too much. Generally speaking, it is unusual to use orders greater than $3$ or $4$ because for large values of $d$, the polynomial curve can become overly flexible and can take on some very strange shapes.

:::: {style="display: grid; grid-template-columns: 1fr 21fr; grid-column-gap: 2px; place-items: start; padding: 2em 2em 2em 2em; border: 5px solid #f8f8f8;"}
::: {}
__R Commands__

```{r poly1_a, echo=1:15}
## Polynomial regression model ##
#
# -- Find the degree of the polynomial through LOOCV 
cverror <- numeric(5)
degrees <- 1:5
for(i in degrees){
  train_control <- trainControl(method = 'LOOCV')
  f <- bquote(medv ~ poly(lstat, .(i), raw = TRUE))
  models <- train(as.formula(f), 
                  data = train.data, 
                  trControl = train_control, 
                  method = 'lm')
  cverror[i] <- (models$results$RMSE)^2
}
# -- Print d and MSE
cbind(degrees, cverror)
```
:::
::: {}
__Comments__

* We create an empty numeric vector, `cverror`, where we will save the LOOCV MSE for each value of degree $d$.
* We give a grid of possible values for $d$ in `degree`, from 1 to 5 (looking at the scatterplot between `lstat` and `medv`, choosing more seems unnatural).
* We use the `caret` package to run the LOOCV:
    * In `trainControl` we specify the model selection method to be used.
    * In `f` we save formulas as `medv ~ poly(lstat, d, raw = T)`, with `d`$= 1,2,3,4,5$.
    * In `models`, we train the models for a specific `d`.
    * The `method` argument in the function `train` represents the function to run the polynomial regression.
    * We extract the MSE from `models` and save its values in the $d$-th element of `cverror`.
* The greatest difference between the various values of `cverror` is between its 1^st^ and 2^nd^ elements, hence between a linear and a parabolic function.
* We select $d^{opt} = 2$.
:::
::::

```{r poly1_b}
# -- The model
d_opt <- 2
model.poly <- lm(medv ~ poly(lstat, d_opt), data = train.data)
summary(model.poly)

# -- The plot
ggplot(train.data, aes(x = lstat, y = medv) ) +
  geom_point(pch = 17, size = 2, col = "darkorange2") +
  labs(x = "% lower status",
       y = "median value of owner-occupied homes ($1000s)") + 
  theme(axis.title.y = element_text(vjust = 3.5),
        axis.title.x = element_text(vjust = -1.5)) + 
  stat_smooth(method = lm, formula = y ~ poly(x, d_opt))
```

> **What can we say about the model?** <br> **How can we interpret the coefficients?**

> Since coefficients in polynomial regression can be estimated easily using ordinary least squares for a multiple linear regression, the interpretation is as before: every $\beta_j$ of the final model represents the average effect of a one-unit increase in `lstat`^j^ on `medv`, holding all other predictors fixed.

We could also have chosen through a comparison of nested models: in this case, we need to calculate the polynomials separately and then combine the comparisons with an `anova` call.

```{r poly1_anova_cv}
tmp.dg1 <- lm(medv ~ lstat, data = train.data)                         # d = 1
tmp.dg2 <- lm(medv ~ poly(lstat, 2, raw = T), data = train.data)       # d = 2
tmp.dg3 <- lm(medv ~ poly(lstat, 3, raw = T), data = train.data)       # d = 3
tmp.dg4 <- lm(medv ~ poly(lstat, 4, raw = T), data = train.data)       # d = 4
tmp.dg5 <- lm(medv ~ poly(lstat, 5, raw = T), data = train.data)       # d = 5
anova(tmp.dg1,tmp.dg2,tmp.dg3,tmp.dg4,tmp.dg5)
```

We use the test set to predict the new `medv` values by `model.poly` and calculate the metrics for the goodness of fit.

```{r poly2}
## Fit the model ##
pred.poly <- model.poly %>% predict(test.data)
```

```{r poly3}
## Model performance ##
performance.poly <- data.frame(
  AIC = round(AIC(model.poly),4),
  BIC = round(BIC(model.poly),4),
  RMSE = RMSE(pred = pred.poly, obs = test.data$medv),
  R2 = R2(pred = pred.poly, obs = test.data$medv),
  MAE = MAE(pred = pred.poly, obs = test.data$medv)
)
```

## Step functions

The model formula is
\[
    \text{medv}_i = \beta_0 + \beta_1 \ C_1(\text{lstat})_i + \beta_2 \ C_2(\text{lstat})_i + \ldots + \beta_K \ C_K(\text{lstat})_i + \varepsilon_i, \qquad i = 1, \dots, n,
\] 
where, for $k = 1, \ldots, K-1$,
\[
    C_k(\text{lstat})_i = \begin{cases}
        1 & \text{lstat}_i \in [c_k; c_{k+1}] \\
        0 & \text{otherwise}
    \end{cases}
\]
and, by convention,
\[
    C_0(\text{lstat})_i = \begin{cases}
        1 & \text{lstat}_i \in (-\infty; c_1] \\
        0 & \text{otherwise}
    \end{cases} \qquad \text{and} \qquad C_K(\text{lstat})_i = \begin{cases}
        1 & \text{lstat}_i \in (c_K, +\infty) \\
        0 & \text{otherwise}.
    \end{cases}
\]

TO sum up, we create cutpoints $c_1, c_2, \dots , c_K$ in the range of `lstat`,
and then construct $K + 1$ new variables.

Since different values of $k$ lead to different $c_k$, hence to different models, we must treat these $k$/$c_k$ parameters as tuning parameters. 

In this case we decide to try different values for $K$, and we choose the optimal $K$, $K^{opt}$ by leave-one-out cross validation (LOOCV) on the training data set; thus, we fit the step regression using $K^{opt}$ as number of cut-points/knots to define the intervals of the step function.

:::: {style="display: grid; grid-template-columns: 1fr 21fr; grid-column-gap: 2px; place-items: start; padding: 2em 2em 2em 2em; border: 5px solid #f8f8f8;"}
::: {}
__R Commands__

```{r step1_a}
## Step regression model ##
#
# -- Find the number of knots through LOOCV 
# -- (1 <= #knots <= 5)
cverror <- numeric(5)
knots <- 2:6
for(i in knots){
  train_control <- trainControl(method = 'LOOCV')
  f <- bquote(medv ~ cut(lstat, .(i)))
  models <- train(as.formula(f), 
                  data = train.data, 
                  trControl = train_control, 
                  method = 'lm')
  cverror[i] <- (models$results$RMSE)^2
}
# -- Print k and MSE
cbind(knots, cverror[-1])
```
:::
::: {}
__Comments__

* The function used to create $C_k(\cdot)$ is `cut`, which second argument being the option `breaks`.
* When `breaks` is specified as a single number, the range of the data is divided into `breaks` pieces of equal length, and then the outer limits are moved away by 0.1% of the range to ensure that the extreme values both fall within the break intervals.
* The procedure is similar at the one we saw to determine $d^{opt}$ for polynomial regression.
* We can choose either $K^{opt} = 5$ or $K^{opt} = 6$, the differences are minimal.
:::
::::

```{r step1_b}
# -- The model
k_opt <- 6
model.step <- lm(medv ~ cut(lstat, k_opt), data = train.data)
summary(model.step)

# -- The plot
ggplot(train.data, aes(x = lstat, y = medv) ) +
  geom_point(pch = 17, size = 2, col = "darkorange2") +
  labs(x = "% lower status",
       y = "median value of owner-occupied homes ($1000s)") + 
  theme(axis.title.y = element_text(vjust = 3.5),
        axis.title.x = element_text(vjust = -1.5)) + 
  stat_smooth(method = lm, formula = y ~ cut(x, k_opt))
```

From the graph above, we can see one of the problems of the step functions: unless there are natural breakpoints in the predictors, piecewise constant functions can miss the interesting data.

Now we try to predict the new values for `medv`.

```{r step1_c}
## Fit the model ##
pred.step <- model.step %>% predict(test.data)
```

The error that appears after `predict` is a very common error which occurs when levels are missing in categorical variables in the test data set: to overcome this problem, we cut the training data to get the necessary knots and we recode `lstat` throught the intervals defined by these knots both in the training and test data sets. 

Only after having discretized `lstat` we can calculate predicted values for `medv` and the metrics on the goodness of fit of the step regression model.

```{r step2_a}
## Cut the training data to get cut points ##
C1 <- cut(train.data$lstat, 6)
levels(C1)
CutPoints = c(-Inf, 7.77, 13.8, 19.9, 25.9, 31.9, Inf)

## New data frames with binned data ##
Binned.training <- train.data
Binned.training$lstat = cut(train.data$lstat, CutPoints)
Binned.test <- test.data
Binned.test$lstat = cut(test.data$lstat, CutPoints)

# -- The model
model.step <- lm(medv ~ lstat, data = Binned.training)
summary(model.step)
```

```{r step2_b}
## Fit the model ##
pred.step <- model.step %>% predict(Binned.test)
```

```{r step3}
## Model performance ##
performance.step <- data.frame(
  AIC = round(AIC(model.step),4),
  BIC = round(BIC(model.step),4),
  RMSE = RMSE(pred = pred.step, obs = Binned.test$medv),
  R2 = R2(pred = pred.step, obs = Binned.test$medv),
  MAE = MAE(pred = pred.step, obs = Binned.test$medv)
)
```

## Regression splines

The model formula is
\[
    \text{medv}_i = \beta_0 + \beta_1 \ \text{lstat}_i + \beta_2 \ \text{lstat}_i^2 + \ldots + \beta_d \ \text{lstat}_i^d + \beta_{d+1} \ h(\text{lstat}, \xi_1)_i + \ldots + \beta_{d+K} \ h(\text{lstat}, \xi_K)_i + \varepsilon_i, \qquad i = 1, \dots, n,
\]
where 
\[
    h(\text{lstat}, \xi_k)_i = (\text{lstat}_i - \xi_k)_+^d = \begin{cases}
        (\text{lstat}_i - \xi_k)^d & \text{lstat}_i > \xi_k \\
        0 & \text{othewise}.
    \end{cases}
\]

To have an idea of what these $h(\cdot, \cdot)$ are doing, let us fix $d = 1$, $\xi_1 = 10$, $\xi_2 = 20$ and $\xi_3 = 30$. Then
\[
    h(\text{lstat}, \xi_1)_i = \begin{cases}
        \text{lstat}_i - 10 & \text{lstat}_i > 10 \\
        0 & \text{othewise}
    \end{cases} 
\] 
\[
    h(\text{lstat}, \xi_2)_i = \begin{cases}
        \text{lstat}_i - 20 & \text{lstat}_i > 20 \\
        0 & \text{othewise}
    \end{cases} 
\] 
\[
    h(\text{lstat}, \xi_3)_i = \begin{cases}
        \text{lstat}_i - 30 & \text{lstat}_i > 30 \\
        0 & \text{othewise}.
    \end{cases}
\]

and plotted all together are as follows.

```{r example}
x <- sort(Boston$lstat)
h1 <- ifelse(x > 10, x-10, 0)
h2 <- ifelse(x > 20, x-20, 0)
h3 <- ifelse(x > 30, x-30, 0)
plot(x, h1, type = "l", lwd = 2, ylab = expression(h(x,xi)))
lines(x, h2, lty = 2, lwd = 2)
lines(x, h3, lty = 3, lwd = 2)
legend("topleft", legend = c(expression(h(x,xi[1]) ~ with ~ xi[1] == 10), 
                             expression(h(x,xi[2]) ~ with ~ xi[2] == 20), 
                             expression(h(x,xi[3]) ~ with ~ xi[3] == 30)), 
       lty = 1:3, lwd = 2, bty = "n")
```

So, in this case we have 2 tuning parameters we have to keep in considerations: the knots $\xi_k$ and the degree of the polynomial bases $d$. Also in this case, the approach is to choose by cross validation, or LOOCV, the optimal $\xi_k$'s and/or $d$.

### Knots fixed

```{r spline1_a, eval = 1:4}
## Regression splines ##
#
# -- Choose the knots 
knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75))
# -- We have to choose also the degree of the polynomial function
?bs
```

```{r spline1_b, echo=FALSE, results = "asis"}
static_help <- function(pkg, topic, out, links = tools::findHTMLlinks()) {
  pkgRdDB = tools:::fetchRdDB(file.path(find.package(pkg), 'help', pkg))
  force(links)
  tools::Rd2HTML(pkgRdDB[[topic]], out, package = pkg,
                 Links = links, no_links = is.null(links))
}
tmp <- tempfile()
static_help("splines", "bs", tmp)
out <- readLines(tmp)
parts <- grep("h3>", out, fixed = T)
cat(out[(parts[2]):(parts[3] - 3)], out[(parts[3]):(parts[4] - 2)], sep = "")
```

As example:

```{r}
out_bs <- bs(train.data$lstat, knots = knots, degree = 2)
```

A matrix of dimension `c(length(train.data$lstat), df)`, where either `df` was supplied or if knots were supplied, `df = length(knots) + degree` plus one if there is an intercept. Attributes are returned that correspond to the arguments to `bs`.

:::: {style="display: grid; grid-template-columns: 1fr 21fr; grid-column-gap: 2px; place-items: start; padding: 2em 2em 2em 2em; border: 5px solid #f8f8f8;"}
::: {}
__R Commands__

```{r spline1_c1, warning=FALSE}
# -- Find the degree of the polynomial through LOOCV 
# -- (2 <= degrees <= 8)
cverror <- numeric(7)
degrees <- 2:8
for(i in degrees){
  f <- bquote(medv ~ bs(lstat, knots = knots, degree = .(i)))
  models <- glm(as.formula(f), data = train.data)
  cverror[i] <- cv.glm(train.data, models)$delta[1]
}
cbind(degrees, cverror[-1])
```
:::
::: {}
__Comments__

* We run the OLS algorithm not through `lm`, but by `glm` function (with default options).
* To apply the cross-validation using 10 folds, we use the `cv.glm` function in the package `boot`. 
* The value of the MSE (averaged over 10-folds CV MSEs) is extracted by the models throught the element `delta`.
* We either choose as optimal $d$ the value $d^{opt} = 2$ or $d^{opt} = 3$.
:::
::::

```{r spline1_c2}
# -- The model
d_opt <- 3
model.spl <- lm(medv ~ bs(lstat, knots = knots, degree = d_opt), data = train.data)
summary(model.spl)
# -- The plot
ggplot(train.data, aes(x = lstat, y = medv) ) +
  geom_point(pch = 17, size = 2, col = "darkorange2") +
  labs(x = "% lower status",
       y = "median value of owner-occupied homes ($1000s)") +
  theme(axis.title.y = element_text(vjust = 3.5),
        axis.title.x = element_text(vjust = -1.5)) +
  stat_smooth(method = lm, formula = y ~ splines::bs(x, df = d_opt))
```

```{r spline2}
## Fit the model ##
pred.spl <- model.spl %>% predict(test.data)
```

```{r spline3}
## Model performance ##
performance.spl <- data.frame(
  AIC = round(AIC(model.spl),4),
  BIC = round(BIC(model.spl),4),
  RMSE = RMSE(pred = pred.spl, obs = test.data$medv),
  R2 = R2(pred = pred.spl, obs = test.data$medv),
  MAE = MAE(pred = pred.spl, obs = test.data$medv)
)
```

### Degree fixed

> ***DIY:*** *Instead of keeping the knots fixed, now choose a grid of possible knots and keep the degree of the polynomial function in the splines fixed to 3 (the default in `bs` -- argument `degree = 3`). Copy and paste the previous code, changing the appropriate parameters.* 


## Generalized additive models (GAMs)

The model formula is
\[
    \text{medv}_i = \beta_0 + f(\text{lstat})_i + \varepsilon_i, \qquad i = 1, \dots, n,
\]
where $f(\text{lstat})$ is a generic smooth non-linear function in `lstat`.

In this case we choose for $f(\text{lstat})$ a cubic regression spline, i.e., we model the following formula:
\[
    \text{medv}_i = \beta_0 + \beta_1 \ \text{lstat}_i + \beta_2 \ \text{lstat}_i^2 + \beta_3 \ \text{lstat}_i^3 + \beta_4 \ h(\text{lstat}, \xi_1)_i + \ldots + \beta_{3+K} \ h(\text{lstat}, \xi_K)_i + \varepsilon_i, \qquad i = 1, \dots, n.
\]

:::: {style="display: grid; grid-template-columns: 3fr 2fr; grid-column-gap: 2px; place-items: start; padding: 2em 2em 2em 2em; border: 5px solid #f8f8f8;"}
::: {}
__R Commands__

```{r gam1_a, fig.height=7}
## GAM regression model ##
#
# -- The model
model.gam <- gam(medv ~ s(lstat, bs = "cr"), 
                 data = train.data)
summary(model.gam)
# -- Plot of 
plot(model.gam)
# -- Diagnostic plots of model
gam.check(model.gam, k.rep = 1000)
```
:::
::: {}
__Comments__

* The function we use to estimate a GAM model is `gam` in `mgcv` package.
* The function `s` defines smooth terms within `gam` model formulae. 
    * We have to pass as its first argument the predictor.
    * `bs` is a two letter character string indicating the (penalized) smoothing basis to use. (e.g., `"tp"` for thin plate regression spline (the default), `"cr"` for cubic regression spline)
* By default, the smoothing parameter estimation method is `method = "GCV.Cp"`. See `?gam` for further details.
* The function `plot` takes a fitted `gam` object produced by `gam()` and plots the component smooth functions that make it up, on the scale of the linear predictor.
* The function `gam.check` takes a fitted `gam` object produced by `gam()` and produces some diagnostic information about the fitting procedure and results. The default is to produce 4 residual plots, some information about the convergence of the smoothness selection optimization, and to run diagnostic tests of whether the basis dimension choises are adequate.
    * The argument `k.rep = 1000` is for how many re-shuffles to do to get $p$-value for $k$ testing. See `?gam.check` for further details.
:::
::::

```{r gam1_b}
# -- The plot
ggplot(train.data, aes(x = lstat, y = medv) ) +
  geom_point(pch = 17, size = 2, col = "darkorange2") +
  labs(x = "% lower status",
       y = "median value of owner-occupied homes ($1000s)") + 
  theme(axis.title.y = element_text(vjust = 3.5),
        axis.title.x = element_text(vjust = -1.5)) + 
  stat_smooth(method = gam, formula = y ~ s(x))
```

```{r gam2}
## Fit the model ##
pred.gam <- model.gam %>% predict(test.data)
```

```{r gam3}
## Model performance ##
performance.gam <- data.frame(
  AIC = round(AIC(model.gam),4),
  BIC = round(BIC(model.gam),4),
  RMSE = RMSE(pred = pred.gam, obs = test.data$medv),
  R2 = R2(pred = pred.gam, obs = test.data$medv),
  MAE = MAE(pred = pred.gam, obs = test.data$medv)
)
```

## Comparing the univariate models

From analyzing the RMSE and the R$^2$ metrics of the different models, it can be seen that the spline regression, the generalized additive models and the loess regression outperform all other approaches.

```{r}
## Put the results together ##
tmp <- cbind(Model = c("LM: `y ~ x`", "POLYNOMIAL: `y ~ poly(x, 2)`", "STEP: `y ~ cut(x, 6)`", 
                       "SPLINES: `y ~ bs(x, 3)`", "GAM: `y ~ s(x)`"), 
             rbind(performance.lm, performance.poly, performance.step, performance.spl, performance.gam))

## Pretty table ##
kable(tmp, row.names = F, col.names = c("*Model*", "AIC", "BIC", "RMSE", "$R^2$", "MAE"), 
      escape = F, align = c("lccccc"), digits = 4) %>%
  kable_styling(full_width = F, bootstrap_options = "responsive")
```

# Models: adding other predictors

Now we want to predict `medv` by `lstat`, `crim`, `rad`, `chas` and the interaction between `lstat` and `chas`.

## Multiple linear model using `gam` function

```{r linear_fit}
## The model ##
linear_fit <- gam(medv ~ lstat + crim + rad + chas + lstat:chas, data = train.data)
summary(linear_fit)

## The predicted values ##
predict_linear <- linear_fit %>% predict(test.data)

## Performance metrics ## 
performance_linear <- data.frame(
  AIC = AIC(linear_fit),
  BIC = BIC(linear_fit),
  RMSE = RMSE(pred = predict_linear, obs = test.data$medv),
  R2 = R2(pred = predict_linear, obs = test.data$medv),
  MAE = MAE(pred = predict_linear, obs = test.data$medv),
  GCV.Cp = linear_fit$gcv.ubre
)
```

## GAM model

```{r gam_fit, fig.height=7}
## The model ##
#
gam_fit <- gam(medv ~ s(lstat) + s(crim) + rad + chas + s(lstat, by = chas), 
               data = train.data)
summary(gam_fit)
# -- Thin plate regression splines components by predictor
plot(gam_fit, scale = 0, shade = T, shade.col = "darkorange2")
#scale = 0 returns different y axis for each plot
#shade = T produce shaded regions as confidence bands for smooths
#seWithMean
# -- Parametric components
termplot(gam_fit)
# Comment: try to see here for interaction terms plotting 
#          https://cran.r-project.org/web/packages/interplot/vignettes/interplot-vignette.html
# -- Diagnostics plots
gam.check(gam_fit)

## The predicted values ##
predict_gam <- gam_fit %>% predict(test.data)

## Performance metrics ## 
performance_gam <- data.frame(
  AIC = AIC(gam_fit),
  BIC = BIC(gam_fit),
  RMSE = RMSE(pred = predict_gam, obs = test.data$medv),
  R2 = R2(pred = predict_gam, obs = test.data$medv),
  MAE = MAE(pred = predict_gam, obs = test.data$medv),
  GCV.Cp = gam_fit$gcv.ubre
)
```

`termplot` shows the contribution of each of the two terms in the model, at the mean of the contributions for the other term.

```{r model_comparison}
## Comparisons ## 
anova(linear_fit, gam_fit, test = "Chisq")
performance_linear
performance_gam
```

> ***What can be said about these 2 models?***


More information about gam: https://www.maths.ed.ac.uk/~swood34/.

# To sum up:

1 . *Polynomial regression* extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. `poly` function.

2 . *Step functions* cut the range of a variable into K distinct regions in
order to produce a qualitative variable. This has the effect of fitting
a piecewise constant function. `cut` function.

3 . *Regression splines* are more flexible than polynomials and step functions, and in fact are an extension of the two. They involve dividing the range of $X$ into $K$ distinct regions. Within each region, a polynomial function is fit to the data. However, these polynomials are constrained so that they join smoothly at the region boundaries, or knots. Provided that the interval is divided into enough regions, this can produce an extremely flexible fit. `bs` function.

4 . *Generalized additive models* allow us to extend the methods above to
deal with multiple predictors. `gam` function.
