---
title: "Exploratory Data Analysis"
subtitle: "How to start with data analysis"
author: "Angela Andreella"
date: ""
output: 
    html_document:
        toc: TRUE
        number_sections: TRUE
        theme: united
        highlight: tango
        css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Exploratory Data Analysis** (EDA) is a crucial part and usually takes up most of the time. A proper and extensive EDA would reveal interesting **patterns** and help to prepare the data in a better way for the following analyses. It can be roughly summarized in three big parts:

1. **Structure and summary of data**: To check the type of variables in the data set and compute location indexes (e.g., mean, median), variability indexes (e.g., variance) of the variables of interest ,

2. **Exploratory plots**: Histograms, box plots, bar plots, correlogram or scatter plots (e.g., skeweness, outliers), 

3. **Preprocessing step**: Are there any `NA`s? Missing values? Integer variables to be converted in factors?  \dots

Before starting the analysis, load the library `tidyverse` which loads directly the packages `ggplot2` and `dplyr`:


```{r}
library(tidyverse)
```

We firstly analyze simulated data, then we will see a real data set example.


Let's **create our data set**. We have $50$ subjects ($25$ with Alzheimer disease and $25$ healthy individuals). We analyze the **response time** (in milliseconds) of the Decoding Test VIPER-NAM (Images will appear on the screen for a short period of time and then disappear. Four letters will then appear, only one of which will correspond to the letter of the object. The user must choose the correct letter as quickly as possible). The response time is collected $10$ **times** for each individual. We also "collect" the **Attention Control Scale** (ATTC, i.e., self-report scale that is designed to measure attention focusing and attention shifting). The ATTC consists of $20$ items (we will consider only one here) that are rated on a four-point Likert scale from $1$ (almost never) to $4$ (always). We also "collect" sex and age for each individual.

Let's create our data:

:::: {.columns}
::: {.column width="60%"}

__R code__

```{r}
set.seed(1234)

generateData <- function(time){
  
  db <- data.frame(Age = sample(c(15:60), 50, replace = TRUE),
                   Sex = sample(c(0, 1), 50, replace = TRUE),
                   Group = c(rep(0, 25),rep(1, 25)))
  
  db <- db %>% mutate(ATTC1 = ifelse(Group == 1, 
                                     sample(c(3,4),1), 
                                     sample(c(1:4),1)),
                      Response_Time = log(Age) * rgamma(50, shape = 300) +
                        log(time) * rgamma(50, shape = 300) + 
                        Sex * rgamma(50, shape = 300) + 
                        Group *rgamma(50, shape = 300) + 
                        log(ATTC1) * rgamma(50, shape = 300))
  
  return(db)
}

db <- sapply(c(1:10), function(x) generateData(x), simplify = FALSE)
db <- bind_rows(db)
db$Time <- rep(1:10, each = 50)
db$ID <- rep(1:50, 10)
```
::: 
::: {.column width="40%"}

__Comment__

<br>

* `set.seed(1234)` -- Since We are using random values generated by `R`, we must set the specify the seed `1234`.

* `sample(c(15:60), 50, replace = TRUE)` -- Here we take a sample of $50$ elements from the vector `c(15:60)`. We set `replace=TRUE` to sampling with replacement.

* `c(rep(0, 25),rep(1, 25))` --  Here We repeat the integer value $0$ $25$ times (i.e., number of healthy individuals), and the integer value $1$ $25$ times (i.e., number of individuals with Alzheimer disease).

* `ifelse(Group == 1, sample(c(3,4),1), sample(c(1:4),1))` -- Here We create an integer vector, where the element takes value in $\{3,4\}$ if the group variable equals $1$ (i.e., case group), and takes value in $\{1,2,3,4\}$ if the group variable equals $0$ (i.e., control group).

* `rgamma(50, shape = 300)` -- Random generation of $50$ values from a Gamma distribution with shape parameter (i.e., the mean in this case) equals $300$ milliseconds.

:::
::::

To have a _realistic_ dataset we want to insert also some outliers and missing values. 

> What is an outlier?

An outlier is a value or an observation that is distant from other observations, that is to say, a data point that differs significantly from other data points. Enderlein (1987) goes even further as the author considers outliers as values that deviate so much from other observations one might suppose a different underlying sampling mechanism.

> How to detect outliers `R`?

- Descriptive statistics,

- Histogram,

- Boxplots,

- Percentiles

> What is a missing value?

A missing value is one whose value is unknown. Missing values are represented in `R` by the `NA` (not available) symbol. `NA` is a special value whose properties are different from other values.

> How to detect missing value in `R`?

- `is.na(x)` where `x` is an `R` object to be tested. It return a logical value: `TRUE` if `x` contains missing values, `FALSE` otherwise.

So, let insert some *bad values* e.g., **outliers** and **missing values**(`NA`) in our dataset to understand how to manage them.

:::: {.columns}
::: {.column width="60%"}

__R code__

```{r}
row_idx <- sample(c(1:500), 2)
db[row_idx, "Response_Time"] <- db[row_idx, "Response_Time"] + 4000

row_idx <- sample(c(1:500), 2)
db[row_idx, "Response_Time"] <- NA
```
:::
::: {.column width="40%"}

__Comment__

<br>

* `db[row_idx, "Response_Time"]` -- Here We select a random sample of $2$ observations (rows) and the `Response_Time` variable (column);

* `db[row_idx, "Response_Time"] + 4000` -- Here We add a large value ($4000$) to these observations, i.e., we create two **outliers**;

* `db[row_idx, "Response_Time"] <- NA` -- Here We replace with `NA` the `Response_Time` variables of the observations indexed by `row_idx`, i.e., we create two **missing values**.

* Note that we run two times the `sample()` function to use different observations for creating the outliers and missing values.

:::
::::

Let modify the `Sex` and `Group` variables with proper values:

```{r}
db <- db %>%
  mutate(Sex = ifelse(Sex == 0, "M", "F"),
         Group = ifelse(Group == "0", "Control", "Case"))
```

Now that we have created our dataset, we save it as a `.csv` file:

:::: {style="display: grid; grid-template-columns: 1fr 21fr; grid-column-gap: 2px; place-items: start; padding: 2em 2em 2em 2em; border: 5px solid #f8f8f8;"}
::: {}

__R code__

```{r}
write.csv(x = db, file = "data/our_first_db.csv",row.names = FALSE)
```
:::
::: {}

__Comment__

<br>

* `write.csv` need two arguments: `x` the object to be written as `.csv` file, and the path `file` where we want to save it.

* Here, We impose `row.names = FALSE`. `row.names` indicates whether the row names of `x` are to be written along with `x`.

:::
::::

and load it back into our session for training `r emo::ji("smile")`:

```{r}
db <- read.csv("data/our_first_db.csv")
```

# Structure and summary of the data

Let see how our data are constructed using some functions presented in the last lesson.

:::: {.columns}
::: {.column width="60%"}

__R code__

```{r, echo=1:80}
str(db) # or db %>% str()
```
::: 
::: {.column width="40%"}

__Comments__

<br> 

* We have two character variables (`Group`, and `Sex`), one numeric (`Response_Time`) and four integer ones (`ID`, `Time`, and `Age`). 

* We can also see the number of observations (`r db %>% nrow()`) and the number of variable (`r db %>% ncol()`).

:::
::::


Let see some **descriptive statistics** for each variable:

::: {.columns}
::: {.column width="60%"}

__R code__

```{r}
summary(db) 
```
:::
::: {.column width="40%"}

__Comments__

<br> 

* We can note that **no descriptive statistics** are computed for the **character** variables.

* For the other ones, `summary()` returns the **minimum**, the **maximum**, the first **quartile**, the median, the third quantile and the **mean**.

* Note that `summary()` shows two **missing values** for the variable `Response_Time`, and maximum equals `r max(db$Response_Time)` along with a minimum equals `r min(db$Response_Time)` that makes us think we are in the presence of **outlier/s**. 

:::
::::

We must transform the character variables as factor, so `R` analyze them as **categorical variables** instead of simple **strings**.


:::: {.columns}
::: {.column width="40%"}

__R code__

```{r}
# Variable "Group"
db$Group <- as.factor(db$Group) 
levels(db$Group)
# Variable "Sex"
db$Sex <- as.factor(db$Sex)
levels(db$Sex)
```
:::
::: {.column width="60%"}

__Comments__

<br>

* `as.factor(x)` -- function transforming variable `x` in a categorical one.

* `levels(x)` -- function listing all levels of variable `x`.

:::
::::

We can use instead of `as.factor` the `dplyr` function `mutate_if()`:

:::: {.columns}
::: {.column width="40%"}

__R code__

```{r}
db <- db %>% mutate_if(is.character, as.factor)
```
::: 
::: {.column width="60%"}

__Comments__

* `mutate_if(., x,y)` -- if `x` is satisfied then `y` is applied on `.`

:::
::::

> What does the dataset structure look like now?

```{r}
str(db)
```

# Exploratory plots

Let now perform some EDA, the main step to understand data before actually building models. 

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("Images/piebert.gif")
```

We will use the `ggplot2` package to create plots, however there are also `base` plotting functions. The most  commonly used plot function in `R` is `plot()` which generates both point and line plots. An example:

```{r, fig.align='center'}
plot(x = db$Age, y = db$Response_Time)
```

where `x` and `y` are the two variables to be plotted. Note that `plot()` directly plot points and not lines by default. To plot lines, add the `type="l"` parameter to the `plot()` function. To plot both points and line, set the type parameter to `"b"` (for both).


## How works `ggplot`?

The main parts of a `ggplot` function are:

1. `data`: the **dataset** that we want to analyze,

2. `mapping`: the **variable(s)** that we want to plot,

3. `geom_`: the **geometric object**, i.e., the visual representations. This part is added to the `ggplot` function using the sign `+`.

There are many geometric objects. The most used ones are:

1. `geom_histogram()`: **histogram plot**

2. `geom_points()`: **scatter plot**

3. `geom_line()`: **line plot** (e.g., spaghetti plot)

4. `geom_boxplot()`: **box-plot**

5. `geom_density()`: **density plot**

6. `geom_bar()`: **bar plot**

## Histogram

For example, let represent the variable `Response_Time` by an histogram:

:::: {.columns}
::: {.column width="60%"}

__R code__

```{r}
ggplot(data = db, 
       mapping = aes(x = Response_Time)) + 
  geom_histogram()
```
:::
::: {.column width="40%"}
__Comments__

* Note that we have two warnings:

  1. `stat_bin() using bins = 30.` 
  `Pick better value with binwidth`: `R` says that the histogram was computed considering $30$ bins (default value). If you want to change it you can set the argument `bins = x` inside the `geom_histogram()` function where `x` is the number of bins.
  
  2. `Warning: Removed 2 rows containing` 
  `non-finite values (stat_bin)`: `R` says that it removes two observations directly (i.e., our missing values).
  
* Note also that the **outliers** are clearly visible in the right part of the histogram.

:::
:::

We can also change the layout of our plot (i.e., color, size, labels etc)

:::: {.columns}
::: {.column width="60%"}
__R code__

```{r, warning = FALSE, comment = FALSE}
ggplot(data = db, 
       mapping = aes(x = Response_Time)) + 
  geom_histogram(fill = "red") + 
  theme_classic() + 
  xlab("Response time")
```
:::
::: {.column width="40%"}
__Comments__

* `color` -- Here We change the color of the histogram considering a character value with the name of the color, see [here](https://r-graph-gallery.com/42-colors-names.html) for an overview  of color names in `R`

* `theme_classic()` -- Here We change the theme of the plot. See [here](https://ggplot2.tidyverse.org/reference/ggtheme.html) for an overview of the ggplot themes.

* `xlab()` -- Here We change the label of the x axis.

:::
::::

We can divide our observations by the `Group` variables and fill the histogram with two different colors:

:::: {.columns}
::: {.column width="60%"}

__R code__
```{r, warning = FALSE, comment = FALSE}
ggplot() + 
  geom_histogram(data = db, aes(x = Response_Time,  
                     fill = Group))+
  geom_vline(data = db, aes(xintercept = mean(Response_Time, na.rm = TRUE)),
              linetype="dashed", size=1)+
  geom_vline(data = db %>% filter(Group =="Case"), 
             aes(xintercept= mean(Response_Time, na.rm = TRUE), 
                 colour =Group),
             linetype="dashed", 
             size=1)+
  geom_vline(data = db %>% filter(Group =="Control"),
             aes(xintercept=mean(Response_Time, na.rm = TRUE), 
                 colour = Group),
             linetype="dashed", 
             size=1)
```

:::
::: {.column width="40%"}

__Comments__

* `fill` -- Here We put the variables that discriminate the observations. `geom_histogram` will plot these observations using different colors.

* `geom_vline()` -- Here We create a vertical line. You must specify the `X` intercept in the argument `xintercept`. Here, we consider the global mean, and the mean for each group. Note that we remove the missing values inside the `mean()` function. Is you do not specify `na.rm = TRUE`, the mean equals `NA`.

* `linetype="dashed"` -- Here We specify the type of line to be used (dashed).

* `size=1` -- Here We specify the size of the line

* Note that here we specify the `data` inside each geometrical objects (before we specified it in the `ggplot()` function directly). This is recommendable when we want to use different geometrical objects in the same plot. In addition, in this way we can filter directly the `data` when we create the two vertical lines for group.

:::
::::


In the same way we can create scatter plots, bar plots, etc.

## Scatter plots

:::: {.columns}
::: {.column width="60%"}

__R code__
```{r}
ggplot(data = db, 
       mapping = aes(x = Age, 
                     y = Response_Time, 
                     color = Group)) + 
  geom_point()
```
:::
::: {.column width="40%"}

__Comments__

* `color` -- Here We put the variable that specify different colors, i.e., the `Group` object

* Note that the two outliers are clearly visible, What group are they from? 

:::
::::

and constructing one different scatter plots divided by sex:

:::: {.columns}
::: {.column width="60%"}

__R code__

```{r}
ggplot(data = db, 
       mapping = aes(x = Age, y = Response_Time, color = Group)) + 
  geom_point() +
  facet_wrap(. ~Sex)+
       scale_colour_discrete(name = "Group", labels = c("Individuals with Alzeihmer disease", "Healthy individuals")) +
  ylab("Response time")
```
:::
::: {.column width="40%"}

__Comments__

* `facet_wrap(. ~Sex)` -- Here we put the variable to divide the scatter plots, i.e., the `Sex` object. `R` direclty creates two plots one for each level of the `Sex` factor object.

* `scale_colour_discrete(name = "Group", ` `labels = c("Individuals with Alzeihmer disease", "Healthy individuals"))` -- Here We can change the labels of the `Group` legend. We must specify the `name` of the object as a string (i.e., `Group`) and the labels desidered as vector.

* `ylab` -- Here We can change the y axis.

:::
::::

## Line plot

Let make some plots to understand individual data.

:::: {.columns}
::: {.column width="60%"}

__R code__

```{r}
ggplot(data = db %>% filter(Time %in% c(1,5,10)), 
       mapping = aes(x = Time, y = Response_Time, group = ID, color = Group)) + 
  geom_line() +
   geom_point(aes(fill=Group),
              shape=21)
```
:::
::: {.column width="40%"}

__Comments__

* `db %>% filter(Time %in% c(1,5, 10)` -- Here We consider only observations at time $1$, $5$ and $10$. If you want all the observations just drop off this part.

* `group = ID, color = Group` -- Here We group the observations by `ID` (i.e., one line for each individual) and We color them by `Group`.

* `geom_point(aes(fill=Group), shape=21)` -- Here we add points for each time. The `shape = 21` specify the type of mark (i.e., point). `fill=Group` colors the points by `Group`.

:::
::::

Let try to plot one line for each mean group and its confidence interval at level $0.95$.

:::: {.columns}
::: {.column width="60%"}

__R code__

```{r}
conf.interval <- 0.95
ci_quantile <- qt(conf.interval/2 + .5, 25-1)
rtwc_between <- db %>% 
  filter(Time %in% c(1,10))%>% 
  group_by(Group, Time) %>%
  summarize(mean = mean(Response_Time, na.rm = TRUE),
            sd = sd(Response_Time, na.rm = TRUE))

ggplot(data = rtwc_between, 
       mapping = aes(x = Time, y = mean, color = Group)) + 
  geom_line() +
   geom_point(aes(fill=Group),
              shape=21) +
  geom_errorbar(aes(ymin=rtwc_between$mean-ci_quantile*rtwc_between$sd/sqrt(25-1), ymax=rtwc_between$mean+ci_quantile*rtwc_between$sd/sqrt(25-1), color = Group), width=.1)
```
:::
::: {.column width="40%"}

__Comments__

* `conf.interval <- 0.95` -- Here we specify the level of the confidence interval.

* `qt(conf.interval/2 + .5, 25-1)` -- Here we compute the quantile of the t distribution at level $0.95/2$ with $25-1$ degree of freedom.

* `geom_errorbar(aes(ymin=rtwc_between$mean` `-ci_quantile*rtwc_between$sd/sqrt(25-1),` `ymax=rtwc_between$mean``+ci_quantile*` `rtwc_between$sd/sqrt(25-1),` `color = Group), width=.1)` -- Here we plot the error bars that represent our confidence intervals. The `geom_errorbar()` function takes the `ymin` and `ymax` arguments (i.e., the minimum and maximum values for the variable in the y axis). Then, We color these intervals by `Group` and We specify the `width` for better visualization.

:::
::::

## Box plot

Let see the distribution of the `Response_Time` variable across different time points.

:::: {.columns}
::: {.column width="60%"}

__R code__

```{r}
ggplot(data = db, 
       mapping = aes(x = Response_Time, group = Time)) + 
  geom_boxplot() + 
  facet_grid(. ~Group)
```
:::
::: {.column width="40%"}

__Comments__

* `group = Time` -- Here We group the observations by `Time`,

* `facet_grid(. ~Group)` -- As before, We divide the plots into two graphs, one for the case group and one for the control group.

* Note the outliers in the right part of the graph.

:::
::::

## Density plot

:::: {.columns}
::: {.column width="60%"}

__R code__

```{r}
ggplot()  +
  geom_density(data = db,
               aes(x = Response_Time, color = Group), size=1) +
  geom_vline(data = db %>% filter(Group == "Case"), 
             aes(xintercept = mean(Response_Time, na.rm = TRUE), 
                 color = Group), size=.8, linetype = "dotted")+
  geom_vline(data = db %>% filter(Group == "Control"), 
             aes(xintercept = mean(Response_Time, na.rm = TRUE), 
                 color = Group), size=.8, linetype = "dotted")+
  geom_vline(data = db, 
             aes(xintercept = mean(Response_Time, na.rm = TRUE)), size=.8, linetype = "dashed") +
    theme_classic()
```
:::
::: {.column width="40%"}

__Comments__

* Note how we have a **positive skew**, i.e., the tail is on the right. This is caused by the presence of outliers.

* Let analyze together the structure of this plot.

:::
::::


## Bar plot

```{r}
ggplot(data = db, 
       mapping = aes(x = Group, fill = Sex)) + 
  geom_bar()
```

> How can we improve it?

```{r}
ggplot(data = db, 
       mapping = aes(x = Group, fill = Sex)) + 
  geom_bar(position="fill")
```

## Correlation plot

We can check the correlation between the *quantitative* variables to look at the level of linear dependence between pairs of two variables. We can either 

(a) Read the correlation matrix (since the correlation matrix is symmetric -- $COR(X,Y) = COR(Y,X)$ --, we can simply look at the upper or lower triangular version of it).

:::: {.columns}
::: {.column width="60%"}
__R commands__ 

```{r correlation1}
# Correlation matrix
corr_matrix <- round(cor(db[,!(colnames(db) %in% c("Sex", "Group", "ID"))], use = "complete.obs"), 2)
corr_matrix[lower.tri(corr_matrix)] <- 0
corr_matrix
```
:::
::: {.column width="40%"}
__Comments__ 

* First line:
  * `!(colnames(db) %in%` `c("Sex", "Group", "ID"))` -- keep only the columns having name different from `Sex`, `Group` and `ID`.
  * `db[,!(...)]` -- keep the data set without `Sex`, `Group` and `ID` columns.
  * `cor(db[...])` -- compute the correlation matrix for all possible couples of variables in the data set considering the complete observations (`use = "complete.obs"`).
  * `round(cor(...), 2)` -- rounds to two decimal places all correlation values.
  
* Second line: keep only the values in the upper triangular part of the matrix for a better reading.
:::
::::

(b) Plot the correlation matrix through a correlogram (higher absolute values of correlation between pairs of variables correspond to more vibrant colors on the cells of the correlogram for those pairs).

:::: {.columns}
::: {.column width="60%"}
__R commands__ 

```{r correlation2, fig.width=14, fig.height=14}
# Correlogram
#install.packages("ggcorrplot")
library(ggcorrplot)                                             
ggcorrplot(corr_matrix, 
           type = "upper", 
           lab = T, 
           lab_size = 7, 
           outline.col = "white", 
           colors = c("tomato2", "white", "springgreen3"), 
           title = "", 
           ggtheme = theme_gray, 
           pch.cex = 30, 
           tl.cex = 20)
```
:::
::: {.column width="40%"}

__Comments__ 

* Install and load the library `ggcorrplot` to plot the correlogram. 
* The function to be used is `ggcorrplot(x)`, where `x` is the correlation matrix (basic usage).
* In particular:
  * `corr_matrix` -- the first argument to be passed is the correlation matrix.
  * `type = "upper"` -- print only the upper triangular part of the matrix.
  * `lab = T` -- add correlation values on the plot. 
  * `lab_size = 7` -- dimension of correlation values on the plot.
  * `outline.col = "white"` -- border color of the cells of the correlation values.
  * `colors = c("tomato2", "white",` `"springgreen3")` -- vector of 3 colors for negative, mid and positive correlation values.
  * `title = ""` -- no title of the plot.
  * `ggtheme = theme_gray` -- ggplot2 function or theme object
  * `pch.cex = 30` -- size of symbol for not statistically significant correlation values.
  * `tl.cex = 20` -- size of variable name labels.
:::
::::

*General note:* If you use a function inside a package only one time, and you don't want to occupy memory when loading all the functions inside that package, once you have installed the latter you can simply call the function of interest by `package::function(...)`. For example, if we only want to use the `ggcorrplot` function from the homonymous package, we can simply use `ggcorrplot::ggcorrplot(x)`.

# Preprocessing steps

Now, we see the final part of this lesson, i.e., the preprocessing step. It is pretty simple and fast in our example, since We directly created our data. However, in real life, this step is generally the most most time-consuming. There are several approaches for dealing with **missing values** and **outliers**. See 

- [Little, R. J., & Rubin, D. B. (2019). Statistical analysis with missing data (Vol. 793). John Wiley & Sons.](https://books.google.it/books?hl=it&lr=&id=BemMDwAAQBAJ&oi=fnd&pg=PR11&dq=Statistical+Analysis+with+Missing+Data&ots=FBEU80KXZX&sig=OUoTlrdvAqv1QjcbEND2_aoCd80#v=onepage&q=Statistical%20Analysis%20with%20Missing%20Data&f=false)

- [Van Buuren, S. (2018). Flexible imputation of missing data. CRC press.](https://books.google.it/books?hl=it&lr=&id=lzb3DwAAQBAJ&oi=fnd&pg=PP1&dq=Flexible+Imputation+of+Missing+Data&ots=Vg5ZZLhdUZ&sig=K3mxkurPPqP4on7FpnEMzTJkPas#v=onepage&q=Flexible%20Imputation%20of%20Missing%20Data&f=false)  

- [Nordhausen, K. (2014). Multiple Imputation and its Application by James R. Carpenter, Michael G. Kenward.](https://onlinelibrary.wiley.com/doi/full/10.1111/insr.12051_13?casa_token=ZXSFLZFYzlkAAAAA%3A7HhUE7VhnZh1GZM-MEeMTLJsRcCnwU4qK9IEReRpe72g5X5EQXMMsLhVZVtmp-ptAEvpAaOGjYWU-Kg) 

Managing missing values is a huge chapter in statistical analysis. This is because before handling missing values, we need to understand **why we have these values**. Are they due to measurement problems? If so, are they **random** or **related to any detected variables**? (e.g., a greater presence in cases than in controls). As we can guess, missing values can also be **informative**, which is the first thing we need to analyze. However, here we simply find these missing values and outliers and eliminate them.

Let see how drop the missing values in `R`:

```{r}
db <- db %>% filter(!is.na(Response_Time))
```

`filter` permits to filter out directly our missing values using `!is.na`, i.e., the negation of the logical values calculated by the `is.na` function. Alternatively, we can use the basic commands of `R`:

```{r}
db <- db[!is.na(db$Response_Time),]
```

Since `!is.na(db$Response_Time)` returns a vector of logical values, we can use it as row indices. `R` will take only the rows corresponding to the `TRUE` value.

Another function:

```{r}
db <- na.omit(db)
```

`na.omit` returns the object with incomplete cases removed.

Let see how dealing with outliers. First of all we need to detect them:

```{r}
which(db$Response_Time > quantile(db$Response_Time, .99))
```
We have `r length(which(db$Response_Time > quantile(db$Response_Time, .99)))` observations that are greater than the $0.99$ quantile of the `Response_Time` variable. The function `which()` return the indices corresponding to a `TRUE` value. Let's filter our dataset:

```{r}
idx <- which(db$Response_Time > quantile(db$Response_Time, .99))
db[idx,]
```

It is pretty obvious that our outliers are the ones from individuals `r db[db$Response_Time > 4000, "ID"]`. We can impute these outliers 

```{r}
db <- db %>% filter(ID %in% c(275, 466))
```

> Let see our EDA after preprocessing step!


# Real data example

Now, we will analyze a real data set. The data comes from the study of Woodworth et al. (2018), a replication of a landmark randomized controlled trial (Seligman et al. 2005) which had suggested that positive psychology interventions, when delivered via the internet, could increase participants' happiness and decrease their depression relative to the changes effected by a placebo control. 

Their main finding was contrary to that of the original study by Seligman et al. (2005). All interventions, including the theoretically-neutral placebo, led to significant increases in happiness and to significant reductions in depression. The effects of the positive-psychology interventions were statistically indistinguishable from those of the placebo. For more details see [here](https://openpsychologydata.metajnl.com/articles/10.5334/jopd.35#B2).


We have two data sets that you can find in the `S3/data` folder.

The first data set (`ahi-cesd.csv`) comprises $992$ point-in-time records of the self-reported happiness and depression of 295 participants, each assigned to one of four intervention groups, in a study of the effect of web-based positive-psychology interventions on happiness and depression. Each point-in-time measurement consists of a participant's responses to the 24 items of the Authentic Happiness Inventory (AHI) and to the 20 items of the Center for Epidemiological Studies Depression (CES-D) scale. Measurements were attempted at the time of each participant's enrollment in the study and on five subsequent occasions, the last being approximately 189 days after enrollment. It contains the following variables:

1. `id`: Participant ID.

2. `occasion`: Measurement occasion:

    * `0` = Pretest (i.e., at enrollment),
  
    * `1` = Posttest (i.e., 7 days after pretest),
  
    * `2` = 1-week follow-up, (i.e., 14 days after pretest, 7 days after posttest),
  
    * `3` = 1-month follow-up, (i.e., 38 days after pretest, 31 days after posttest),
  
    * `4` = 3-month follow-up, (i.e., 98 days after pretest, 91 days after posttest),
  
    * `5` = 6-month follow-up, (i.e., 189 days after pretest, 182 days after posttest).

3. `elapsed.days`: Time since enrollment measured in fractional days.

4. `intervention`: $3$ positive psychology interventions (PPIs), plus 1 control condition:

    * `1` = Using signature strengths,
    * `2` = Three good things,
    * `3` = Gratitude visit,
    * `4` = Recording early memories (control condition).

5. `ahi01`-`ahi24`: Responses on 24 AHI items.

6. `cesd01`-`cesd20`: Responses on 20 CES-D items.

7. `ahiTotal`: Total AHI score.

8. `cesdTotal`: Total CES-D score.

The second dataset (`participant-info.csv`) contains demographic information about the each of the 295 participants. The data are suitable for various time-series analyses and between-group comparisons. It contains the following variables:

1. `id`: Participant's ID.

2. `intervention`: $3$ positive psychology interventions (PPIs), plus 1 control condition:

    * `1` = Using signature strengths,
    * `2` = Three good things,
    * `3` = Gratitude visit,
    * `4` = Recording early memories (control condition).

3. `sex`: `1` for female, `2` for female

4. `age` : Participant's age (in years).

5. `educ`: Level of education: 

    * `1` = Less than Year 12,
    * `2` = Year 12,
    * `3` = Vocational training,
    * `4` = Bachelor's degree,
    * `5` = Postgraduate degree.

6. `income`:

    * `1` = below average,
    * `2` = average,
    * `3` = above average.

Let's load our data sets:

```{r}
db <- read.csv("data/ahi-cesd.csv")
db_part <- read.csv("data/participant-info.csv")
```


First of all, we must join the two datasets together. We use the `tidytable` package (which we already load through the `tidyverse` package). There are several ways to join two dataset:

- Considering one data set we add the other one to the left (i.e., we consider the observations in the first data set): `left_join`
- Considering one data set we add the other one to the right (i.e., we consider the observations in the second data set): `right_join`
- Considering both data sets we take the intersection (i.e., observations that are in both data sets in the same time): `inner_join`
 - Considering both data sets we take the union (i.e., observations from all data sets): `full_join`


```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("Images/joins.png")
```

```{r}
db_full <- tidytable::inner_join(db, db_part, by = c("id", "intervention"))
```


```{r}
dim(db)
dim(db_part)
dim(db_full)
```

Now, let's see the structure of our dataset:

```{r}
str(db_full)
```
We must make some preprocessing:

- Transform some variable as factor ones: `occasion`, `intervention`, `sex`, `educ`, and `income`

- Check for `NA`s and/or outliers

```{r}

ahi_var <- colnames(db_full)[grepl("ahi", colnames(db_full))]
cesd_var <- colnames(db_full)[grepl("cesd", colnames(db_full))]



db_full <- db_full %>%
  dplyr::select(-c(ahi_var[-25], cesd_var[-21])) %>%
  mutate(occasion = as.factor(occasion),
         intervention = as.factor(intervention),
         sex = as.factor(sex),
         educ = as.factor(educ),
         income = as.factor(income))
```

```{r}
sum(is.na(db_full))
```
```{r}
summary(db_full)
```

Now, we can create some exploratory plots. Let's see the distribution of the total AHI score divided by type of intervention:

```{r}
ggplot(db_full) +
  geom_boxplot(aes(y = ahiTotal, x = intervention))
```

However, we want to see if the total AHI score increases after the intervention. For simplicity, let's consider the first and second occasions (i.e., `occasion` equals `0` and `1`):

```{r}
db_full %>%
  filter(occasion %in% c(0,1)) %>%
ggplot() +
  geom_boxplot(aes(y = ahiTotal, x = intervention:occasion, fill = occasion))
```

We can note an increment of the total AHI score for all the type of intervetion.

Let's analyze the total CESD one:

```{r}
db_full %>%
  filter(occasion %in% c(0,1)) %>%
ggplot() +
  geom_boxplot(aes(y = cesdTotal, x = intervention:occasion, fill = occasion))
```

Here, we can see a reduction of the total CESD score for all type of psychological intervention.

Here, we can see a relationship between the total AHI score and CESD one:

```{r}
ggplot(db_full) +
  geom_point(aes(x = ahiTotal, y = cesdTotal, group = id)) +
  geom_smooth(aes(x = ahiTotal, y = cesdTotal))
```
So, high values of AHI correspond to low value of CESD in general.

Another interesting point is to see the total AHI score for each timepoint and each participants and the mean for each intervention:


```{r}
db_full %>%
  group_by(intervention, occasion) %>%
  mutate(mean_ahiTotal = mean(ahiTotal)) %>%
  ggplot() + 
  geom_line(aes(x = occasion, y = ahiTotal, group = id)) +
    geom_line(aes(x=occasion, 
           y=mean_ahiTotal, 
           group=intervention,
           colour=intervention), size=1.5) 
```

or considering directly the global mean for each occasion and interventation with corresponding $0.95$ confidence intervals:

```{r}
db_full %>%
  group_by(intervention, occasion) %>%
  mutate(mean_ahiTotal = mean(ahiTotal),
         sd_ahiTotal=sd(ahiTotal),
         n_ahiTotal=length(ahiTotal),
         upper=mean_ahiTotal+sd_ahiTotal/sqrt(n_ahiTotal),
         lower=mean_ahiTotal-sd_ahiTotal/sqrt(n_ahiTotal)) %>%
ggplot() + 
    geom_line(aes(x=occasion, 
           y=mean_ahiTotal, 
           group=intervention,
           colour=intervention), size=1.5) +
  geom_errorbar(aes(x=occasion, ymin=upper, ymax=lower), 
                width=0.2, size=1, color="grey",alpha=.5)
```

> Do you have any graphics in mind? Try it yourself and ask if you are in any difficulty

Let's save our preprocessed data set as `RData` file for the next lesson:

```{r}
save(db_full, file = "../S4/data/db_full.RData")
```


# Concluding


> How can I save plots?

There are many ways to save a `ggplot`:

- Go to the bottom right windows of `RStudio`. `Plots > Export > Save as PDF`,

- Using the `ggsave("r-graphics.pdf", plot_obj)` function from the `ggplot2` package, where `r-graphics.pdf` is the file name to create on disk and `plot_obj` is the plot to save.

- Open a graphic device, i.e., `pdf("r-graphics.pdf")`, create and print a plot, and finally close the graphic device using the function `dev.off()`.

# References

- [ggplot cookbook](http://www.cookbook-r.com/Graphs/)

- [ggplot online documentation](http://docs.ggplot2.org/current/)

Woodworth, R.J., O'Brien-Malone, A., Diamond, M.R. and Schüz, B., 2018. Data from, 'Web-based Positive Psychology Interventions: A Reexamination of Effectiveness'.  Journal of Open Psychology Data,  6(1), p.1.DOI: https://doi.org/10.5334/jopd.35

Seligman, M E, Steen, T A, Park, N and Peterson, C 2005 Positive psychology progress: Empirical validation of interventions. American Psychologist, 60(5): 410-421. DOI: https://doi.org/10.1037/0003-066X.60.5.410
